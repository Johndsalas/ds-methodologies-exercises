{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the case, department, and source data into their own spark dataframes.\n",
    "\n",
    "Let's see how writing to the local disk works in spark:\n",
    "\n",
    "Write the code necessary to store the source data in both csv and json format, store these as sources_csv and sources_json\n",
    "\n",
    "Inspect your folder structure. What do you notice?\n",
    "\n",
    "Inspect the data in your dataframes. Are the data types appropriate? Write the code necessary to cast the values to the appropriate types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = spark.read.csv(\"source_copy.csv\", sep=\",\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "department = spark.read.csv(\"dept_copy.csv\", sep=\",\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "case = spark.read.csv(\"case_copy.csv\", sep=\",\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------------+\n",
      "|source_id| source_username|\n",
      "+---------+----------------+\n",
      "|   100137|Merlene Blodgett|\n",
      "|   103582|     Carmen Cura|\n",
      "|   106463| Richard Sanchez|\n",
      "|   119403|  Betty De Hoyos|\n",
      "|   119555|  Socorro Quiara|\n",
      "+---------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "source.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_source_jason = source.write.json(\"sources_json\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_source_csv = source.write.csv(\"sources_csv\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('source_id', 'string'), ('source_username', 'string')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('dept_division', 'string'),\n",
       " ('dept_name', 'string'),\n",
       " ('standardized_dept_name', 'string'),\n",
       " ('dept_subject_to_SLA', 'string')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "department.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('case_id', 'int'),\n",
       " ('case_opened_date', 'string'),\n",
       " ('case_closed_date', 'string'),\n",
       " ('SLA_due_date', 'string'),\n",
       " ('case_late', 'string'),\n",
       " ('num_days_late', 'double'),\n",
       " ('case_closed', 'string'),\n",
       " ('dept_division', 'string'),\n",
       " ('service_request_type', 'string'),\n",
       " ('SLA_days', 'double'),\n",
       " ('case_status', 'string'),\n",
       " ('source_id', 'string'),\n",
       " ('request_address', 'string'),\n",
       " ('council_district', 'int')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "case.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "case = case.withColumn(\"case_closed\", expr('case_closed == \"YES\"')).withColumn(\n",
    "    \"case_late\", expr('case_late == \"YES\"')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "case = case.withColumnRenamed(\"SLA_due_date\", \"case_due_date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmt = \"M/d/yy H:mm\"\n",
    "\n",
    "case = (\n",
    "    case.withColumn(\"case_opened_date\", to_timestamp(\"case_opened_date\", fmt))\n",
    "    .withColumn(\"case_closed_date\", to_timestamp(\"case_closed_date\", fmt))\n",
    "    .withColumn(\"case_due_date\", to_timestamp(\"case_due_date\", fmt))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('case_id', 'int'),\n",
       " ('case_opened_date', 'timestamp'),\n",
       " ('case_closed_date', 'timestamp'),\n",
       " ('case_due_date', 'timestamp'),\n",
       " ('case_late', 'boolean'),\n",
       " ('num_days_late', 'double'),\n",
       " ('case_closed', 'boolean'),\n",
       " ('dept_division', 'string'),\n",
       " ('service_request_type', 'string'),\n",
       " ('SLA_days', 'double'),\n",
       " ('case_status', 'string'),\n",
       " ('source_id', 'string'),\n",
       " ('request_address', 'string'),\n",
       " ('council_district', 'int')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "case.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How old is the latest (in terms of days past SLA) currently open issue? \n",
    "\n",
    "How long has the oldest (in terms of days since opened) currently opened issue been open?\n",
    "\n",
    "How many Stray Animal cases are there?\n",
    "\n",
    "How many service requests that are assigned to the Field Operations department (dept_division) are not classified as \"Officer Standby\" request type (service_request_type)?\n",
    "\n",
    "Convert the council_district column to a string column.\n",
    "\n",
    "Extract the year from the case_closed_date column.\n",
    "Convert num_days_late from days to hours in new columns num_hours_late.\n",
    "\n",
    "Join the case data with the source and department data.\n",
    "\n",
    "Are there any cases that do not have a request source?\n",
    "\n",
    "What are the top 10 service request types in terms of number of requests?\n",
    "\n",
    "What are the top 10 service request types in terms of average days late?\n",
    "Does number of days late depend on department?\n",
    "How do number of days late depend on department and request type?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import concat, sum, avg, min, max, count, mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|max(SLA_days)|\n",
      "+-------------+\n",
      "|   1419.00191|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "case.filter(case.case_closed== False).select(max(case.SLA_days)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'astype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-4cef0d82b8e2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m case.select(\n\u001b[1;32m      2\u001b[0m             datediff(\n\u001b[0;32m----> 3\u001b[0;31m                       \u001b[0mcase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcase_closed\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcase_opened_date\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'string'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m                       \u001b[0mcase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcase_closed_date\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"string\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                     )\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.7/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1299\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1300\u001b[0m             raise AttributeError(\n\u001b[0;32m-> 1301\u001b[0;31m                 \"'%s' object has no attribute '%s'\" % (self.__class__.__name__, name))\n\u001b[0m\u001b[1;32m   1302\u001b[0m         \u001b[0mjc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'astype'"
     ]
    }
   ],
   "source": [
    "case.select(\n",
    "            datediff(\n",
    "                      case.filter(case.case_closed==False).select(min(case.case_opened_date)),\n",
    "                      case.select(max(case.case_closed_date))\n",
    "                    )\n",
    "          ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0   2017-01-01 13:48:00\n",
      "Name: date1, dtype: datetime64[ns]\n",
      "0   2018-08-08 10:38:00\n",
      "Name: date2, dtype: datetime64[ns]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Invalid argument, not a string or column: 0   2017-01-01 13:48:00\nName: date1, dtype: datetime64[ns] of type <class 'pandas.core.series.Series'>. For column literals, use 'lit', 'array', 'struct' or 'create_map' function.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-df0623c8e38c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdate2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mdatediff\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdate1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdate2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.7/site-packages/pyspark/sql/functions.py\u001b[0m in \u001b[0;36mdatediff\u001b[0;34m(end, start)\u001b[0m\n\u001b[1;32m   1065\u001b[0m     \"\"\"\n\u001b[1;32m   1066\u001b[0m     \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1067\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatediff\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_to_java_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_to_java_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1068\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1069\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.7/site-packages/pyspark/sql/column.py\u001b[0m in \u001b[0;36m_to_java_column\u001b[0;34m(col)\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;34m\"{0} of type {1}. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0;34m\"For column literals, use 'lit', 'array', 'struct' or 'create_map' \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m             \"function.\".format(col, type(col)))\n\u001b[0m\u001b[1;32m     54\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mjcol\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Invalid argument, not a string or column: 0   2017-01-01 13:48:00\nName: date1, dtype: datetime64[ns] of type <class 'pandas.core.series.Series'>. For column literals, use 'lit', 'array', 'struct' or 'create_map' function."
     ]
    }
   ],
   "source": [
    "#How long has the oldest (in terms of days since opened) currently opened issue been open?\n",
    "start = case.filter(case.case_closed==False).select(min(case.case_opened_date).alias('date1')).toPandas()\n",
    "end = case.select(max(case.case_closed_date).alias('date2')).toPandas()\n",
    "\n",
    "\n",
    "print(start.date1)\n",
    "print(end.date2)\n",
    "\n",
    "datediff(start.date1,end.date2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many service requests that are assigned to the Field Operations department (dept_division) are not classified as \"Officer Standby\" request type (service_request_type)?\n",
    "case.filter(case.dept_division=='Field Operations').filter(case.service_request_type!=\"Officer Standby\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many Stray Animal cases are there?\n",
    "case.select(case.service_request_type=='Stray Animal').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the council_district column to a string column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "case = case.withColumn('council_distrtict',case.council_district.cast(\"string\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the year from the case_closed_date column. \n",
    "\n",
    "Convert num_days_late from days to hours in new columns num_hours_late."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "case.select(case.case_closed_date[1:4].alias('year')).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "case.withColumn('num_days_late', expr('num_days_late * 24 AS num_hours_late')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "case.select(\"num_hours_late\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
