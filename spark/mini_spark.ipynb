{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Spark Dataframe Basics\n",
    "\n",
    "    1. Use the starter code above to create a pandas dataframe.\n",
    "    1. Convert the pandas dataframe to a spark dataframe. From this point\n",
    "       forward, do all of your work with the spark dataframe, not the pandas\n",
    "       dataframe.\n",
    "    1. Show the first 3 rows of the dataframe.\n",
    "    1. Show the first 7 rows of the dataframe.\n",
    "    1. View a summary of the data using `.describe`.\n",
    "    1. Use `.select` to create a new dataframe with just the `n` and `abool`\n",
    "       columns. View the first 5 rows of this dataframe.\n",
    "    1. Use `.select` to create a new dataframe with just the `group` and `abool`\n",
    "       columns. View the first 5 rows of this dataframe.\n",
    "    1. Use `.select` to create a new dataframe with the `group` column and the\n",
    "       `abool` column renamed to `a_boolean_value`. Show the first 3 rows of\n",
    "       this dataframe.\n",
    "    1. Use `.select` to create a new dataframe with the `group` column and the\n",
    "       `n` column renamed to `a_numeric_value`. Show the first 6 rows of this\n",
    "       dataframe.\n",
    "\n",
    "1. Column Manipulation\n",
    "\n",
    "    1. Use the starter code above to re-create a spark dataframe. Store the\n",
    "       spark dataframe in a varaible named `df`\n",
    "\n",
    "    1. Use `.select` to add 4 to the `n` column. Show the results.\n",
    "\n",
    "    1. Subtract 5 from the `n` column and view the results.\n",
    "\n",
    "    1. Multiply the `n` column by 2. View the results along with the original\n",
    "       numbers.\n",
    "\n",
    "    1. Add a new column named `n2` that is the `n` value multiplied by -1. Show\n",
    "       the first 4 rows of your dataframe. You should see the original `n` value\n",
    "       as well as `n2`.\n",
    "\n",
    "    1. Add a new column named `n3` that is the n value squared. Show the first 5\n",
    "       rows of your dataframe. You should see both `n`, `n2`, and `n3`.\n",
    "\n",
    "    1. What happens when you run the code below?\n",
    "\n",
    "        ```python\n",
    "        df.group + df.abool\n",
    "        ```\n",
    "\n",
    "    1. What happens when you run the code below? What is the difference between\n",
    "       this and the previous code sample?\n",
    "\n",
    "        ```python\n",
    "        df.select(df.group + df.abool)\n",
    "        ```\n",
    "\n",
    "    1. Try adding various other columns together. What are the results of\n",
    "       combining the different data types?\n",
    "\n",
    "1. Spark SQL\n",
    "\n",
    "    1. Use the starter code above to re-create a spark dataframe.\n",
    "    1. Turn your dataframe into a table that can be queried with spark SQL. Name\n",
    "       the table `my_df`. Answer the rest of the questions in this section with\n",
    "       a spark sql query (`spark.sql`) against `my_df`. After each step, view\n",
    "       the first 7 records from the dataframe.\n",
    "    1. Write a query that shows all of the columns from your dataframe.\n",
    "    1. Write a query that shows just the `n` and `abool` columns from the\n",
    "       dataframe.\n",
    "    1. Write a query that shows just the `n` and `group` columns. Rename the\n",
    "       `group` column to `g`.\n",
    "    1. Write a query that selects `n`, and creates two new columns: `n2`, the\n",
    "       original `n` values halved, and `n3`: the original n values minus 1.\n",
    "    1. What happens if you make a SQL syntax error in your query?\n",
    "\n",
    "1. Type casting\n",
    "\n",
    "    1. Use the starter code above to re-create a spark dataframe.\n",
    "\n",
    "    1. Use `.printSchema` to view the datatypes in your dataframe.\n",
    "\n",
    "    1. Use `.dtypes` to view the datatypes in your dataframe.\n",
    "\n",
    "    1. What is the difference between the two code samples below?\n",
    "\n",
    "        ```python\n",
    "        df.abool.cast('int')\n",
    "        ```\n",
    "\n",
    "        ```python\n",
    "        df.select(df.abool.cast('int')).show()\n",
    "        ```\n",
    "\n",
    "    1. Use `.select` and `.cast` to convert the `abool` column to an integer\n",
    "       type. View the results.\n",
    "    1. Convert the `group` column to a integer data type and view the results.\n",
    "       What happens?\n",
    "    1. Convert the `n` column to a integer data type and view the results. What\n",
    "       happens?\n",
    "    1. Convert the `abool` column to a string data type and view the results.\n",
    "       What happens?\n",
    "\n",
    "1. Built-in Functions\n",
    "\n",
    "    1. Use the starter code above to re-create a spark dataframe.\n",
    "    1. Import the necessary functions from `pyspark.sql.functions`\n",
    "    1. Find the highest `n` value.\n",
    "    1. Find the lowest `n` value.\n",
    "    1. Find the average `n` value.\n",
    "    1. Use `concat` to change the `group` column to say, e.g. \"Group: x\" or\n",
    "       \"Group: y\"\n",
    "    1. Use `concat` to combine the `n` and `group` columns to produce results\n",
    "       that look like this: \"x: -1.432\" or \"z: 2.352\"\n",
    "\n",
    "1. Filter / Where\n",
    "\n",
    "    1. Use the starter code above to re-create a spark dataframe.\n",
    "    1. Use `.filter` or `.where` to select just the rows where the group is `y`\n",
    "       and view the results.\n",
    "    1. Select just the columns where the `abool` column is false and view the\n",
    "       results.\n",
    "    1. Find the columns where the `group` column is *not* `y`.\n",
    "    1. Find the columns where `n` is positive.\n",
    "    1. Find the columns where `abool` is true and the `group` column is `z`.\n",
    "    1. Find the columns where `abool` is true or the `group` column is `z`.\n",
    "    1. Find the columns where `abool` is false and `n` is less than 1\n",
    "    1. Find the columns where `abool` is false or `n` is less than 1\n",
    "\n",
    "1. When / Otherwise\n",
    "\n",
    "    1. Use the starter code above to re-create a spark dataframe.\n",
    "    1. Use `when` and `.otherwise` to create a column that contains the text \"It\n",
    "       is true\" when `abool` is true and \"It is false\"\" when `abool` is false.\n",
    "    1. Create a column that contains 0 if n is less than 0, otherwise, the\n",
    "       original n value.\n",
    "\n",
    "1. Sorting\n",
    "\n",
    "    1. Use the starter code above to re-create a spark dataframe.\n",
    "    1. Sort by the `n` value.\n",
    "    1. Sort by the `group` value, both ascending and descending.\n",
    "    1. Sort by the group value first, then, within each group, sort by `n`\n",
    "       value.\n",
    "    1. Sort by `abool`, `group`, and `n`. Does it matter in what order you\n",
    "       specify the columns when sorting?\n",
    "\n",
    "1. Aggregating\n",
    "\n",
    "    1. What is the average `n` value for each group in the `group` column?\n",
    "    1. What is the maximum `n` value for each group in the `group` column?\n",
    "    1. What is the minimum `n` value by `abool`?\n",
    "    1. What is the average `n` value for each unique combination of the `group`\n",
    "       and `abool` column?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark Dataframe Basics\n",
    "\n",
    "Use the starter code above to create a pandas dataframe.\n",
    "\n",
    "Convert the pandas dataframe to a spark dataframe. From this point forward, do all of your work with the spark \n",
    "\n",
    "dataframe, not the pandas dataframe.\n",
    "\n",
    "Show the first 3 rows of the dataframe.\n",
    "\n",
    "Show the first 7 rows of the dataframe.\n",
    "\n",
    "View a summary of the data using .describe.\n",
    "\n",
    "Use .select to create a new dataframe with just the n and abool columns. View the first 5 rows of this dataframe.\n",
    "\n",
    "Use .select to create a new dataframe with just the group and abool columns. View the first 5 rows of this dataframe.\n",
    "\n",
    "Use .select to create a new dataframe with the group column and the abool column renamed to a_boolean_value. Show the \n",
    "first 3 rows of this dataframe.\n",
    "\n",
    "Use .select to create a new dataframe with the group column and the n column renamed to a_numeric_value. Show the \n",
    "first 6 rows of this dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_dataframe = pd.DataFrame(\n",
    "    {\n",
    "        \"n\": np.random.randn(20),\n",
    "        \"group\": np.random.choice(list(\"xyz\"), 20),\n",
    "        \"abool\": np.random.choice([True, False], 20),\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = pyspark.sql.SparkSession.builder.getOrCreate()\n",
    "df = spark.createDataFrame(pandas_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----+-----+\n",
      "|                 n|group|abool|\n",
      "+------------------+-----+-----+\n",
      "|0.8129371988209155|    x|false|\n",
      "|0.0910914505128047|    y| true|\n",
      "|1.5453366753509763|    y|false|\n",
      "+------------------+-----+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+-----+\n",
      "|                   n|group|abool|\n",
      "+--------------------+-----+-----+\n",
      "|  0.8129371988209155|    x|false|\n",
      "|  0.0910914505128047|    y| true|\n",
      "|  1.5453366753509763|    y|false|\n",
      "| -0.3311382914531184|    x|false|\n",
      "|  0.9313433201098019|    y| true|\n",
      "|-0.24070753303643708|    x|false|\n",
      "|   1.087093882125782|    z| true|\n",
      "+--------------------+-----+-----+\n",
      "only showing top 7 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+-----+\n",
      "|summary|                  n|group|\n",
      "+-------+-------------------+-----+\n",
      "|  count|                 20|   20|\n",
      "|   mean|0.13397793943323427| null|\n",
      "| stddev| 0.8529083906856362| null|\n",
      "|    min|-1.7447150950876138|    x|\n",
      "|    max| 1.5453366753509763|    z|\n",
      "+-------+-------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+\n",
      "|                  n|abool|\n",
      "+-------------------+-----+\n",
      "| 0.8129371988209155|false|\n",
      "| 0.0910914505128047| true|\n",
      "| 1.5453366753509763|false|\n",
      "|-0.3311382914531184|false|\n",
      "| 0.9313433201098019| true|\n",
      "+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_n_abool = df.select('n','abool')\n",
    "df_n_abool.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|group|abool|\n",
      "+-----+-----+\n",
      "|    x|false|\n",
      "|    y| true|\n",
      "|    y|false|\n",
      "|    x|false|\n",
      "|    y| true|\n",
      "+-----+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_group_abool = df.select('group','abool')\n",
    "df_group_abool.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|group| True|\n",
      "+-----+-----+\n",
      "|    x|false|\n",
      "|    y| true|\n",
      "|    y|false|\n",
      "+-----+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_group_abool = df.select('group', df.abool.alias('True'))\n",
    "df_group_abool.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "|  1|    2|\n",
      "+---+-----+\n",
      "|  x|false|\n",
      "|  y| true|\n",
      "|  y|false|\n",
      "|  x|false|\n",
      "|  y| true|\n",
      "|  x|false|\n",
      "+---+-----+\n",
      "only showing top 6 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_group_abool = df.select(df.group.alias('1'),df.abool.alias('2'))\n",
    "df_group_abool.show(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Column Manipulation\n",
    "\n",
    "    1. Use the starter code above to re-create a spark dataframe. Store the\n",
    "       spark dataframe in a varaible named `df`\n",
    "\n",
    "    1. Use `.select` to add 4 to the `n` column. Show the results.\n",
    "\n",
    "    1. Subtract 5 from the `n` column and view the results.\n",
    "\n",
    "    1. Multiply the `n` column by 2. View the results along with the original\n",
    "       numbers.\n",
    "\n",
    "    1. Add a new column named `n2` that is the `n` value multiplied by -1. Show\n",
    "       the first 4 rows of your dataframe. You should see the original `n` value\n",
    "       as well as `n2`.\n",
    "\n",
    "    1. Add a new column named `n3` that is the n value squared. Show the first 5\n",
    "       rows of your dataframe. You should see both `n`, `n2`, and `n3`.\n",
    "\n",
    "    1. What happens when you run the code below?\n",
    "\n",
    "        ```python\n",
    "        df.group + df.abool\n",
    "        ```\n",
    "\n",
    "    1. What happens when you run the code below? What is the difference between\n",
    "       this and the previous code sample?\n",
    "\n",
    "        ```python\n",
    "        df.select(df.group + df.abool)\n",
    "        ```\n",
    "\n",
    "    1. Try adding various other columns together. What are the results of\n",
    "       combining the different data types?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, expr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_dataframe = pd.DataFrame(\n",
    "    {\n",
    "        \"n\": np.random.randn(20),\n",
    "        \"group\": np.random.choice(list(\"xyz\"), 20),\n",
    "        \"abool\": np.random.choice([True, False], 20),\n",
    "    }\n",
    ")\n",
    "\n",
    "spark = pyspark.sql.SparkSession.builder.getOrCreate()\n",
    "df = spark.createDataFrame(pandas_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+-----+\n",
      "|                  n|group|abool|\n",
      "+-------------------+-----+-----+\n",
      "|-0.8961118796640158|    z| true|\n",
      "|0.25273982979614745|    z|false|\n",
      "|-0.6453890479473521|    y| true|\n",
      "|0.48466929140525594|    z|false|\n",
      "|-0.2435548348504187|    z| true|\n",
      "+-------------------+-----+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+-----+\n",
      "|                   n|group|abool|\n",
      "+--------------------+-----+-----+\n",
      "| -0.8961118796640158|    z| true|\n",
      "| 0.25273982979614745|    z|false|\n",
      "| -0.6453890479473521|    y| true|\n",
      "| 0.48466929140525594|    z|false|\n",
      "| -0.2435548348504187|    z| true|\n",
      "| -0.6636257694233848|    z| true|\n",
      "|  1.8949633752749824|    y| true|\n",
      "| 0.03879929562778278|    z| true|\n",
      "|  -0.212870179251579|    x| true|\n",
      "|  0.6384613196359048|    z|false|\n",
      "| 0.16481129926672627|    x| true|\n",
      "| -1.0426632033218726|    y|false|\n",
      "| -0.6602088500965239|    y| true|\n",
      "|  0.8626465826209326|    z| true|\n",
      "| -0.7096744823521267|    z| true|\n",
      "| -1.2397470112347688|    y| true|\n",
      "|  1.4006783999609003|    z|false|\n",
      "|   0.429832197003883|    y|false|\n",
      "|-0.40382180555849034|    y| true|\n",
      "| 0.09906442811221136|    y|false|\n",
      "+--------------------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_add_4 = df.select(df.n + 4, df.group, df.abool)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+-----+\n",
      "|                  n|group|abool|\n",
      "+-------------------+-----+-----+\n",
      "|-0.3284239368477828|    y|false|\n",
      "|-1.1388677916321555|    y|false|\n",
      "|0.13133236983850768|    z| true|\n",
      "|  1.499089613929301|    x|false|\n",
      "| -1.716993796330484|    x|false|\n",
      "+-------------------+-----+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sub_5 = df.select(df.n - 5, df.group, df.abool)\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+\n",
      "|            (n * 2)|                  n|\n",
      "+-------------------+-------------------+\n",
      "|-1.7922237593280317|-0.8961118796640158|\n",
      "| 0.5054796595922949|0.25273982979614745|\n",
      "|-1.2907780958947042|-0.6453890479473521|\n",
      "| 0.9693385828105119|0.48466929140525594|\n",
      "|-0.4871096697008374|-0.2435548348504187|\n",
      "+-------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_X2 = df.select(df.n * 2, df.n)\n",
    "df_X2.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Spark SQL\n",
    "\n",
    "    1. Use the starter code above to re-create a spark dataframe.\n",
    "    1. Turn your dataframe into a table that can be queried with spark SQL. Name\n",
    "       the table `my_df`. Answer the rest of the questions in this section with\n",
    "       a spark sql query (`spark.sql`) against `my_df`. After each step, view\n",
    "       the first 7 records from the dataframe.\n",
    "    1. Write a query that shows all of the columns from your dataframe.\n",
    "    1. Write a query that shows just the `n` and `abool` columns from the\n",
    "       dataframe.\n",
    "    1. Write a query that shows just the `n` and `group` columns. Rename the\n",
    "       `group` column to `g`.\n",
    "    1. Write a query that selects `n`, and creates two new columns: `n2`, the\n",
    "       original `n` values halved, and `n3`: the original n values minus 1.\n",
    "    1. What happens if you make a SQL syntax error in your query?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_dataframe = pd.DataFrame(\n",
    "    {\n",
    "        \"n\": np.random.randn(20),\n",
    "        \"group\": np.random.choice(list(\"xyz\"), 20),\n",
    "        \"abool\": np.random.choice([True, False], 20),\n",
    "    }\n",
    ")\n",
    "\n",
    "spark = pyspark.sql.SparkSession.builder.getOrCreate()\n",
    "df = spark.createDataFrame(pandas_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"my_df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+-----+\n",
      "|                  n|group|abool|\n",
      "+-------------------+-----+-----+\n",
      "|-0.3284239368477828|    y|false|\n",
      "|-1.1388677916321555|    y|false|\n",
      "|0.13133236983850768|    z| true|\n",
      "|  1.499089613929301|    x|false|\n",
      "| -1.716993796330484|    x|false|\n",
      "| 0.5960840206563159|    y| true|\n",
      "|-0.5554940205485847|    y|false|\n",
      "+-------------------+-----+-----+\n",
      "only showing top 7 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''SELECT * FROM my_df''').show(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+\n",
      "|                  n|abool|\n",
      "+-------------------+-----+\n",
      "|-0.3284239368477828|false|\n",
      "|-1.1388677916321555|false|\n",
      "|0.13133236983850768| true|\n",
      "|  1.499089613929301|false|\n",
      "| -1.716993796330484|false|\n",
      "| 0.5960840206563159| true|\n",
      "|-0.5554940205485847|false|\n",
      "+-------------------+-----+\n",
      "only showing top 7 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''SELECT n, abool\n",
    "             FROM my_df''').show(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---+\n",
      "|                  n|  g|\n",
      "+-------------------+---+\n",
      "|-0.3284239368477828|  y|\n",
      "|-1.1388677916321555|  y|\n",
      "|0.13133236983850768|  z|\n",
      "|  1.499089613929301|  x|\n",
      "| -1.716993796330484|  x|\n",
      "| 0.5960840206563159|  y|\n",
      "|-0.5554940205485847|  y|\n",
      "+-------------------+---+\n",
      "only showing top 7 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''SELECT n, group AS g\n",
    "             FROM my_df''').show(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a query that selects n, and creates two new columns: n2, the original n values halved, and n3: the original n values minus 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+--------------------+\n",
      "|                  n|                 2n|                  3n|\n",
      "+-------------------+-------------------+--------------------+\n",
      "|-0.3284239368477828|-0.1642119684238914| -1.3284239368477828|\n",
      "|-1.1388677916321555|-0.5694338958160777| -2.1388677916321557|\n",
      "|0.13133236983850768|0.06566618491925384| -0.8686676301614923|\n",
      "|  1.499089613929301| 0.7495448069646505| 0.49908961392930107|\n",
      "| -1.716993796330484| -0.858496898165242| -2.7169937963304838|\n",
      "| 0.5960840206563159|0.29804201032815797|-0.40391597934368406|\n",
      "|-0.5554940205485847|-0.2777470102742923| -1.5554940205485845|\n",
      "+-------------------+-------------------+--------------------+\n",
      "only showing top 7 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "          SELECT n, (n/2) AS 2n, (n-1) AS 3n\n",
    "          FROM my_df\n",
    "          ''').show(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Type casting\n",
    "\n",
    "    1. Use the starter code above to re-create a spark dataframe.\n",
    "\n",
    "    1. Use `.printSchema` to view the datatypes in your dataframe.\n",
    "\n",
    "    1. Use `.dtypes` to view the datatypes in your dataframe.\n",
    "\n",
    "    1. What is the difference between the two code samples below?\n",
    "\n",
    "        ```python\n",
    "        df.abool.cast('int')\n",
    "        ```\n",
    "\n",
    "        ```python\n",
    "        df.select(df.abool.cast('int')).show()\n",
    "        ```\n",
    "\n",
    "    1. Use `.select` and `.cast` to convert the `abool` column to an integer\n",
    "       type. View the results.\n",
    "    1. Convert the `group` column to a integer data type and view the results.\n",
    "       What happens?\n",
    "    1. Convert the `n` column to a integer data type and view the results. What\n",
    "       happens?\n",
    "    1. Convert the `abool` column to a string data type and view the results.\n",
    "       What happens?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_dataframe = pd.DataFrame(\n",
    "    {\n",
    "        \"n\": np.random.randn(20),\n",
    "        \"group\": np.random.choice(list(\"xyz\"), 20),\n",
    "        \"abool\": np.random.choice([True, False], 20),\n",
    "    }\n",
    ")\n",
    "\n",
    "spark = pyspark.sql.SparkSession.builder.getOrCreate()\n",
    "df = spark.createDataFrame(pandas_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- n: double (nullable = true)\n",
      " |-- group: string (nullable = true)\n",
      " |-- abool: boolean (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('n', 'double'), ('group', 'string'), ('abool', 'boolean')]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<b'CAST(abool AS INT)'>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.abool.cast('int') # changes 'abool' to int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|abool|\n",
      "+-----+\n",
      "|    0|\n",
      "|    1|\n",
      "|    1|\n",
      "|    0|\n",
      "|    0|\n",
      "+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df.abool.cast('int')).show(5) # shows abool as int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|abool|\n",
      "+-----+\n",
      "|    0|\n",
      "|    1|\n",
      "|    1|\n",
      "|    0|\n",
      "|    0|\n",
      "+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df.abool.cast('int')).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|group|\n",
      "+-----+\n",
      "| null|\n",
      "| null|\n",
      "| null|\n",
      "| null|\n",
      "| null|\n",
      "+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df.group.cast('int')).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|  n|\n",
      "+---+\n",
      "|  2|\n",
      "|  1|\n",
      "|  0|\n",
      "| -1|\n",
      "|  0|\n",
      "+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df.n.cast('int')).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Built-in Functions\n",
    "\n",
    "    1. Use the starter code above to re-create a spark dataframe.\n",
    "    1. Import the necessary functions from `pyspark.sql.functions`\n",
    "    1. Find the highest `n` value.\n",
    "    1. Find the lowest `n` value.\n",
    "    1. Find the average `n` value.\n",
    "    1. Use `concat` to change the `group` column to say, e.g. \"Group: x\" or\n",
    "       \"Group: y\"\n",
    "    1. Use `concat` to combine the `n` and `group` columns to produce results\n",
    "       that look like this: \"x: -1.432\" or \"z: 2.352\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_dataframe = pd.DataFrame(\n",
    "    {\n",
    "        \"n\": np.random.randn(20),\n",
    "        \"group\": np.random.choice(list(\"xyz\"), 20),\n",
    "        \"abool\": np.random.choice([True, False], 20),\n",
    "    }\n",
    ")\n",
    "\n",
    "spark = pyspark.sql.SparkSession.builder.getOrCreate()\n",
    "df = spark.createDataFrame(pandas_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import concat,sum,avg,min,max,lit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----------------+--------------------+\n",
      "|            max(n)|           min(n)|              avg(n)|\n",
      "+------------------+-----------------+--------------------+\n",
      "|2.1413073401362026|-1.94783532978488|-0.10206248711230792|\n",
      "+------------------+-----------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(max(df.n),min(df.n),avg(df.n)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+\n",
      "|concat(Group: , group)|\n",
      "+----------------------+\n",
      "|              Group: x|\n",
      "|              Group: x|\n",
      "|              Group: z|\n",
      "|              Group: z|\n",
      "|              Group: y|\n",
      "+----------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(concat(lit(\"Group: \"), col('group'))).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "| concat(group, :, n)|\n",
      "+--------------------+\n",
      "|x:-0.952918867797...|\n",
      "|x:-1.304955973401164|\n",
      "|z:-0.009396764764...|\n",
      "|z:2.1413073401362026|\n",
      "|y:0.1305368565456...|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(concat(col('group'),lit(':'),col('n'))).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+-----+\n",
      "|                   n|group|abool|\n",
      "+--------------------+-----+-----+\n",
      "| -0.9529188677978936|    x|false|\n",
      "|  -1.304955973401164|    x| true|\n",
      "|-0.00939676476426...|    z| true|\n",
      "|  2.1413073401362026|    z|false|\n",
      "| 0.13053685654566957|    y| true|\n",
      "+--------------------+-----+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Filter / Where\n",
    "\n",
    "    1. Use the starter code above to re-create a spark dataframe.\n",
    "    1. Use `.filter` or `.where` to select just the rows where the group is `y`\n",
    "       and view the results.\n",
    "    1. Select just the columns where the `abool` column is false and view the\n",
    "       results.\n",
    "    1. Find the columns where the `group` column is *not* `y`.\n",
    "    1. Find the columns where `n` is positive.\n",
    "    1. Find the columns where `abool` is true and the `group` column is `z`.\n",
    "    1. Find the columns where `abool` is true or the `group` column is `z`.\n",
    "    1. Find the columns where `abool` is false and `n` is less than 1\n",
    "    1. Find the columns where `abool` is false or `n` is less than 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_dataframe = pd.DataFrame(\n",
    "    {\n",
    "        \"n\": np.random.randn(20),\n",
    "        \"group\": np.random.choice(list(\"xyz\"), 20),\n",
    "        \"abool\": np.random.choice([True, False], 20),\n",
    "    }\n",
    ")\n",
    "\n",
    "spark = pyspark.sql.SparkSession.builder.getOrCreate()\n",
    "df = spark.createDataFrame(pandas_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+-----+\n",
      "|                   n|group|abool|\n",
      "+--------------------+-----+-----+\n",
      "|-0.24551557663067314|    y| true|\n",
      "| -0.3190973152521853|    y|false|\n",
      "|  0.1989570248777289|    y| true|\n",
      "|   3.245525595148494|    y| true|\n",
      "|-0.22879499425780644|    y|false|\n",
      "+--------------------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter((df.group=='y')).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+-----+\n",
      "|                  n|group|abool|\n",
      "+-------------------+-----+-----+\n",
      "|-1.1900708111273732|    x|false|\n",
      "| 1.2661843729863633|    x|false|\n",
      "|  0.752264034289916|    z|false|\n",
      "| -2.046618168943991|    x|false|\n",
      "| 2.2104831525546276|    x|false|\n",
      "+-------------------+-----+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter((df.abool==False)).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+-----+\n",
      "|                  n|group|abool|\n",
      "+-------------------+-----+-----+\n",
      "|-0.5485559118903058|    z| true|\n",
      "|-1.1900708111273732|    x|false|\n",
      "| 0.4567140114878692|    z| true|\n",
      "|-1.5220834337154558|    z| true|\n",
      "| 1.2661843729863633|    x|false|\n",
      "+-------------------+-----+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter((df.group!='y')).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+-----+\n",
      "|                  n|group|abool|\n",
      "+-------------------+-----+-----+\n",
      "| 0.4567140114878692|    z| true|\n",
      "| 1.2661843729863633|    x|false|\n",
      "|  0.752264034289916|    z|false|\n",
      "|0.13113489992163438|    x| true|\n",
      "| 2.2104831525546276|    x|false|\n",
      "+-------------------+-----+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter((df.n>=0)).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+-----+\n",
      "|                   n|group|abool|\n",
      "+--------------------+-----+-----+\n",
      "| -0.5485559118903058|    z| true|\n",
      "|  0.4567140114878692|    z| true|\n",
      "| -1.5220834337154558|    z| true|\n",
      "|-0.37414105666343084|    z| true|\n",
      "| -1.3918938736350355|    z| true|\n",
      "+--------------------+-----+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df.abool==True).where(df.group=='z').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+-----+\n",
      "|                   n|group|abool|\n",
      "+--------------------+-----+-----+\n",
      "| -0.5485559118903058|    z| true|\n",
      "|  0.4567140114878692|    z| true|\n",
      "| -1.5220834337154558|    z| true|\n",
      "|   0.752264034289916|    z|false|\n",
      "|-0.37414105666343084|    z| true|\n",
      "| -1.3918938736350355|    z| true|\n",
      "|-0.24551557663067314|    y| true|\n",
      "| 0.13113489992163438|    x| true|\n",
      "|  1.3659199127626693|    z|false|\n",
      "| -0.8058204846847192|    z| true|\n",
      "|  0.1989570248777289|    y| true|\n",
      "|   3.245525595148494|    y| true|\n",
      "|   1.573575091944084|    z| true|\n",
      "|-0.22931907951190852|    x| true|\n",
      "+--------------------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter((df.abool==True) | (df.group=='z')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+-----+\n",
      "|                   n|group|abool|\n",
      "+--------------------+-----+-----+\n",
      "| -1.1900708111273732|    x|false|\n",
      "|   0.752264034289916|    z|false|\n",
      "|  -2.046618168943991|    x|false|\n",
      "| -0.3190973152521853|    y|false|\n",
      "|-0.22879499425780644|    y|false|\n",
      "+--------------------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter((df.abool==False) & (df.n < 1)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+-----+\n",
      "|                  n|group|abool|\n",
      "+-------------------+-----+-----+\n",
      "|-0.5485559118903058|    z| true|\n",
      "|-1.1900708111273732|    x|false|\n",
      "| 0.4567140114878692|    z| true|\n",
      "|-1.5220834337154558|    z| true|\n",
      "| 1.2661843729863633|    x|false|\n",
      "+-------------------+-----+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter((df.abool==False) | (df.n < 1)).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. When / Otherwise\n",
    "\n",
    "    1. Use the starter code above to re-create a spark dataframe.\n",
    "    1. Use `when` and `.otherwise` to create a column that contains the text \"It\n",
    "       is true\" when `abool` is true and \"It is false\"\" when `abool` is false.\n",
    "    1. Create a column that contains 0 if n is less than 0, otherwise, the\n",
    "       original n value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_dataframe = pd.DataFrame(\n",
    "    {\n",
    "        \"n\": np.random.randn(20),\n",
    "        \"group\": np.random.choice(list(\"xyz\"), 20),\n",
    "        \"abool\": np.random.choice([True, False], 20),\n",
    "    }\n",
    ")\n",
    "\n",
    "spark = pyspark.sql.SparkSession.builder.getOrCreate()\n",
    "df = spark.createDataFrame(pandas_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+\n",
      "|abool|Truth_String|\n",
      "+-----+------------+\n",
      "| true|  It is True|\n",
      "| true|  It is True|\n",
      "| true|  It is True|\n",
      "|false| It is False|\n",
      "|false| It is False|\n",
      "+-----+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df.abool,when(df.abool==True, \"It is True\").otherwise(\"It is False\").alias('Truth_String')).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+\n",
      "|                   n|   Positive_or_Zero|\n",
      "+--------------------+-------------------+\n",
      "|   0.965877955460664|  0.965877955460664|\n",
      "| -1.2222063988118825|                0.0|\n",
      "|   1.253240449412012|  1.253240449412012|\n",
      "|  0.4160728747383398| 0.4160728747383398|\n",
      "| -0.4075229755234006|                0.0|\n",
      "|  1.2087422998322255| 1.2087422998322255|\n",
      "|-0.39408031374251906|                0.0|\n",
      "| 0.40297869020310917|0.40297869020310917|\n",
      "| -1.4146750338670233|                0.0|\n",
      "|-0.14875979705925765|                0.0|\n",
      "+--------------------+-------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df.n,when(df.n < 0, 0).otherwise(df.n).alias('Positive_or_Zero')).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Sorting\n",
    "\n",
    "    1. Use the starter code above to re-create a spark dataframe.\n",
    "    1. Sort by the `n` value.\n",
    "    1. Sort by the `group` value, both ascending and descending.\n",
    "    1. Sort by the group value first, then, within each group, sort by `n`\n",
    "       value.\n",
    "    1. Sort by `abool`, `group`, and `n`. Does it matter in what order you\n",
    "       specify the columns when sorting?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_dataframe = pd.DataFrame(\n",
    "    {\n",
    "        \"n\": np.random.randn(20),\n",
    "        \"group\": np.random.choice(list(\"xyz\"), 20),\n",
    "        \"abool\": np.random.choice([True, False], 20),\n",
    "    }\n",
    ")\n",
    "\n",
    "spark = pyspark.sql.SparkSession.builder.getOrCreate()\n",
    "df = spark.createDataFrame(pandas_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+-----+\n",
      "|                  n|group|abool|\n",
      "+-------------------+-----+-----+\n",
      "|-1.1515085443856319|    z| true|\n",
      "| -1.009589708385093|    x|false|\n",
      "|-0.6148472378529147|    y|false|\n",
      "|-0.5959164594823737|    z|false|\n",
      "|-0.4759578214832985|    x| true|\n",
      "+-------------------+-----+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.sort(col('n')).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+-----+\n",
      "|                  n|group|abool|\n",
      "+-------------------+-----+-----+\n",
      "|0.40688412957396614|    x|false|\n",
      "|0.25598638794647194|    x| true|\n",
      "|-0.2818700233336481|    x|false|\n",
      "|0.14754486978665704|    x| true|\n",
      "| -1.009589708385093|    x|false|\n",
      "+-------------------+-----+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.sort(col('group')).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+-----+\n",
      "|                   n|group|abool|\n",
      "+--------------------+-----+-----+\n",
      "|  0.4577372907299206|    z|false|\n",
      "|  1.7678038635362425|    z|false|\n",
      "|-0.12281083393578104|    z| true|\n",
      "| -0.5959164594823737|    z|false|\n",
      "| -1.1515085443856319|    z| true|\n",
      "+--------------------+-----+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.sort(col('group').desc()).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+-----+\n",
      "|                  n|group|abool|\n",
      "+-------------------+-----+-----+\n",
      "| -1.009589708385093|    x|false|\n",
      "|-0.4759578214832985|    x| true|\n",
      "|-0.2818700233336481|    x|false|\n",
      "|-0.2450330659435416|    x|false|\n",
      "|0.14754486978665704|    x| true|\n",
      "+-------------------+-----+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.sort(df.group, df.n).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+-----+\n",
      "|                   n|group|abool|\n",
      "+--------------------+-----+-----+\n",
      "|  -1.009589708385093|    x|false|\n",
      "| -0.2818700233336481|    x|false|\n",
      "| -0.2450330659435416|    x|false|\n",
      "| 0.40688412957396614|    x|false|\n",
      "| -0.6148472378529147|    y|false|\n",
      "|0.012962846084642822|    y|false|\n",
      "| 0.39964809161079323|    y|false|\n",
      "|   0.431183579580352|    y|false|\n",
      "|  1.7756123786034375|    y|false|\n",
      "| -0.5959164594823737|    z|false|\n",
      "|  0.4577372907299206|    z|false|\n",
      "|  1.7678038635362425|    z|false|\n",
      "| -0.4759578214832985|    x| true|\n",
      "| 0.14754486978665704|    x| true|\n",
      "| 0.25598638794647194|    x| true|\n",
      "|-0.27437754919665003|    y| true|\n",
      "|-0.21332970863234177|    y| true|\n",
      "|  0.4248849348398329|    y| true|\n",
      "| -1.1515085443856319|    z| true|\n",
      "|-0.12281083393578104|    z| true|\n",
      "+--------------------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.sort(df.abool, df.group, df.n).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+-----+\n",
      "|                   n|group|abool|\n",
      "+--------------------+-----+-----+\n",
      "|  -1.009589708385093|    x|false|\n",
      "| -0.2818700233336481|    x|false|\n",
      "| -0.2450330659435416|    x|false|\n",
      "| 0.40688412957396614|    x|false|\n",
      "| -0.4759578214832985|    x| true|\n",
      "| 0.14754486978665704|    x| true|\n",
      "| 0.25598638794647194|    x| true|\n",
      "| -0.6148472378529147|    y|false|\n",
      "|0.012962846084642822|    y|false|\n",
      "| 0.39964809161079323|    y|false|\n",
      "|   0.431183579580352|    y|false|\n",
      "|  1.7756123786034375|    y|false|\n",
      "|-0.27437754919665003|    y| true|\n",
      "|-0.21332970863234177|    y| true|\n",
      "|  0.4248849348398329|    y| true|\n",
      "| -0.5959164594823737|    z|false|\n",
      "|  0.4577372907299206|    z|false|\n",
      "|  1.7678038635362425|    z|false|\n",
      "| -1.1515085443856319|    z| true|\n",
      "|-0.12281083393578104|    z| true|\n",
      "+--------------------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.sort(df.group, df.abool, df.n).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Aggregating\n",
    "\n",
    "    1. What is the average `n` value for each group in the `group` column?\n",
    "    1. What is the maximum `n` value for each group in the `group` column?\n",
    "    1. What is the minimum `n` value by `abool`?\n",
    "    1. What is the average `n` value for each unique combination of the `group`\n",
    "       and `abool` column?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_dataframe = pd.DataFrame(\n",
    "    {\n",
    "        \"n\": np.random.randn(20),\n",
    "        \"group\": np.random.choice(list(\"xyz\"), 20),\n",
    "        \"abool\": np.random.choice([True, False], 20),\n",
    "    }\n",
    ")\n",
    "\n",
    "spark = pyspark.sql.SparkSession.builder.getOrCreate()\n",
    "df = spark.createDataFrame(pandas_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|group|              avg(n)|\n",
      "+-----+--------------------+\n",
      "|    x| 0.12734729726412766|\n",
      "|    z|-0.37026669931343775|\n",
      "|    y| 0.20798019748146832|\n",
      "+-----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(df.group).agg(avg(df.n)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------+\n",
      "|group|            max(n)|\n",
      "+-----+------------------+\n",
      "|    x|0.6420752442289691|\n",
      "|    z|0.9809816957432566|\n",
      "|    y|1.9171665474777455|\n",
      "+-----+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(df.group).agg(max(df.n)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+--------------------+\n",
      "|group|abool|              avg(n)|\n",
      "+-----+-----+--------------------+\n",
      "|    z|false|-0.30246470831506317|\n",
      "|    y|false| 0.33441380209377886|\n",
      "|    y| true|-0.10810381404930802|\n",
      "|    x|false| 0.42572970273978394|\n",
      "|    x| true| -0.2704959100367474|\n",
      "|    z| true| -0.4380686903118123|\n",
      "+-----+-----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(df.group,df.abool).agg(avg(df.n)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
