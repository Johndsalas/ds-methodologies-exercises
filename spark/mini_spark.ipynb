{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Spark Dataframe Basics\n",
    "\n",
    "    1. Use the starter code above to create a pandas dataframe.\n",
    "    1. Convert the pandas dataframe to a spark dataframe. From this point\n",
    "       forward, do all of your work with the spark dataframe, not the pandas\n",
    "       dataframe.\n",
    "    1. Show the first 3 rows of the dataframe.\n",
    "    1. Show the first 7 rows of the dataframe.\n",
    "    1. View a summary of the data using `.describe`.\n",
    "    1. Use `.select` to create a new dataframe with just the `n` and `abool`\n",
    "       columns. View the first 5 rows of this dataframe.\n",
    "    1. Use `.select` to create a new dataframe with just the `group` and `abool`\n",
    "       columns. View the first 5 rows of this dataframe.\n",
    "    1. Use `.select` to create a new dataframe with the `group` column and the\n",
    "       `abool` column renamed to `a_boolean_value`. Show the first 3 rows of\n",
    "       this dataframe.\n",
    "    1. Use `.select` to create a new dataframe with the `group` column and the\n",
    "       `n` column renamed to `a_numeric_value`. Show the first 6 rows of this\n",
    "       dataframe.\n",
    "\n",
    "1. Column Manipulation\n",
    "\n",
    "    1. Use the starter code above to re-create a spark dataframe. Store the\n",
    "       spark dataframe in a varaible named `df`\n",
    "\n",
    "    1. Use `.select` to add 4 to the `n` column. Show the results.\n",
    "\n",
    "    1. Subtract 5 from the `n` column and view the results.\n",
    "\n",
    "    1. Multiply the `n` column by 2. View the results along with the original\n",
    "       numbers.\n",
    "\n",
    "    1. Add a new column named `n2` that is the `n` value multiplied by -1. Show\n",
    "       the first 4 rows of your dataframe. You should see the original `n` value\n",
    "       as well as `n2`.\n",
    "\n",
    "    1. Add a new column named `n3` that is the n value squared. Show the first 5\n",
    "       rows of your dataframe. You should see both `n`, `n2`, and `n3`.\n",
    "\n",
    "    1. What happens when you run the code below?\n",
    "\n",
    "        ```python\n",
    "        df.group + df.abool\n",
    "        ```\n",
    "\n",
    "    1. What happens when you run the code below? What is the difference between\n",
    "       this and the previous code sample?\n",
    "\n",
    "        ```python\n",
    "        df.select(df.group + df.abool)\n",
    "        ```\n",
    "\n",
    "    1. Try adding various other columns together. What are the results of\n",
    "       combining the different data types?\n",
    "\n",
    "1. Spark SQL\n",
    "\n",
    "    1. Use the starter code above to re-create a spark dataframe.\n",
    "    1. Turn your dataframe into a table that can be queried with spark SQL. Name\n",
    "       the table `my_df`. Answer the rest of the questions in this section with\n",
    "       a spark sql query (`spark.sql`) against `my_df`. After each step, view\n",
    "       the first 7 records from the dataframe.\n",
    "    1. Write a query that shows all of the columns from your dataframe.\n",
    "    1. Write a query that shows just the `n` and `abool` columns from the\n",
    "       dataframe.\n",
    "    1. Write a query that shows just the `n` and `group` columns. Rename the\n",
    "       `group` column to `g`.\n",
    "    1. Write a query that selects `n`, and creates two new columns: `n2`, the\n",
    "       original `n` values halved, and `n3`: the original n values minus 1.\n",
    "    1. What happens if you make a SQL syntax error in your query?\n",
    "\n",
    "1. Type casting\n",
    "\n",
    "    1. Use the starter code above to re-create a spark dataframe.\n",
    "\n",
    "    1. Use `.printSchema` to view the datatypes in your dataframe.\n",
    "\n",
    "    1. Use `.dtypes` to view the datatypes in your dataframe.\n",
    "\n",
    "    1. What is the difference between the two code samples below?\n",
    "\n",
    "        ```python\n",
    "        df.abool.cast('int')\n",
    "        ```\n",
    "\n",
    "        ```python\n",
    "        df.select(df.abool.cast('int')).show()\n",
    "        ```\n",
    "\n",
    "    1. Use `.select` and `.cast` to convert the `abool` column to an integer\n",
    "       type. View the results.\n",
    "    1. Convert the `group` column to a integer data type and view the results.\n",
    "       What happens?\n",
    "    1. Convert the `n` column to a integer data type and view the results. What\n",
    "       happens?\n",
    "    1. Convert the `abool` column to a string data type and view the results.\n",
    "       What happens?\n",
    "\n",
    "1. Built-in Functions\n",
    "\n",
    "    1. Use the starter code above to re-create a spark dataframe.\n",
    "    1. Import the necessary functions from `pyspark.sql.functions`\n",
    "    1. Find the highest `n` value.\n",
    "    1. Find the lowest `n` value.\n",
    "    1. Find the average `n` value.\n",
    "    1. Use `concat` to change the `group` column to say, e.g. \"Group: x\" or\n",
    "       \"Group: y\"\n",
    "    1. Use `concat` to combine the `n` and `group` columns to produce results\n",
    "       that look like this: \"x: -1.432\" or \"z: 2.352\"\n",
    "\n",
    "1. Filter / Where\n",
    "\n",
    "    1. Use the starter code above to re-create a spark dataframe.\n",
    "    1. Use `.filter` or `.where` to select just the rows where the group is `y`\n",
    "       and view the results.\n",
    "    1. Select just the columns where the `abool` column is false and view the\n",
    "       results.\n",
    "    1. Find the columns where the `group` column is *not* `y`.\n",
    "    1. Find the columns where `n` is positive.\n",
    "    1. Find the columns where `abool` is true and the `group` column is `z`.\n",
    "    1. Find the columns where `abool` is true or the `group` column is `z`.\n",
    "    1. Find the columns where `abool` is false and `n` is less than 1\n",
    "    1. Find the columns where `abool` is false or `n` is less than 1\n",
    "\n",
    "1. When / Otherwise\n",
    "\n",
    "    1. Use the starter code above to re-create a spark dataframe.\n",
    "    1. Use `when` and `.otherwise` to create a column that contains the text \"It\n",
    "       is true\" when `abool` is true and \"It is false\"\" when `abool` is false.\n",
    "    1. Create a column that contains 0 if n is less than 0, otherwise, the\n",
    "       original n value.\n",
    "\n",
    "1. Sorting\n",
    "\n",
    "    1. Use the starter code above to re-create a spark dataframe.\n",
    "    1. Sort by the `n` value.\n",
    "    1. Sort by the `group` value, both ascending and descending.\n",
    "    1. Sort by the group value first, then, within each group, sort by `n`\n",
    "       value.\n",
    "    1. Sort by `abool`, `group`, and `n`. Does it matter in what order you\n",
    "       specify the columns when sorting?\n",
    "\n",
    "1. Aggregating\n",
    "\n",
    "    1. What is the average `n` value for each group in the `group` column?\n",
    "    1. What is the maximum `n` value for each group in the `group` column?\n",
    "    1. What is the minimum `n` value by `abool`?\n",
    "    1. What is the average `n` value for each unique combination of the `group`\n",
    "       and `abool` column?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark Dataframe Basics\n",
    "\n",
    "Use the starter code above to create a pandas dataframe.\n",
    "\n",
    "Convert the pandas dataframe to a spark dataframe. From this point forward, do all of your work with the spark \n",
    "\n",
    "dataframe, not the pandas dataframe.\n",
    "\n",
    "Show the first 3 rows of the dataframe.\n",
    "\n",
    "Show the first 7 rows of the dataframe.\n",
    "\n",
    "View a summary of the data using .describe.\n",
    "\n",
    "Use .select to create a new dataframe with just the n and abool columns. View the first 5 rows of this dataframe.\n",
    "\n",
    "Use .select to create a new dataframe with just the group and abool columns. View the first 5 rows of this dataframe.\n",
    "\n",
    "Use .select to create a new dataframe with the group column and the abool column renamed to a_boolean_value. Show the \n",
    "first 3 rows of this dataframe.\n",
    "\n",
    "Use .select to create a new dataframe with the group column and the n column renamed to a_numeric_value. Show the \n",
    "first 6 rows of this dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_dataframe = pd.DataFrame(\n",
    "    {\n",
    "        \"n\": np.random.randn(20),\n",
    "        \"group\": np.random.choice(list(\"xyz\"), 20),\n",
    "        \"abool\": np.random.choice([True, False], 20),\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = pyspark.sql.SparkSession.builder.getOrCreate()\n",
    "df = spark.createDataFrame(pandas_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+-----+\n",
      "|                  n|group|abool|\n",
      "+-------------------+-----+-----+\n",
      "|-0.5935304728163509|    y|false|\n",
      "|-0.6537654714845256|    z| true|\n",
      "| 1.1565401871181058|    y|false|\n",
      "+-------------------+-----+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+-----+\n",
      "|                   n|group|abool|\n",
      "+--------------------+-----+-----+\n",
      "| -0.5935304728163509|    y|false|\n",
      "| -0.6537654714845256|    z| true|\n",
      "|  1.1565401871181058|    y|false|\n",
      "|-0.23502098371277275|    y| true|\n",
      "| -0.7460054809035613|    z| true|\n",
      "| -0.5952652598384565|    z|false|\n",
      "|  0.9394948411503021|    y| true|\n",
      "+--------------------+-----+-----+\n",
      "only showing top 7 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+-----+\n",
      "|summary|                  n|group|\n",
      "+-------+-------------------+-----+\n",
      "|  count|                 20|   20|\n",
      "|   mean|0.14878403127458903| null|\n",
      "| stddev|  0.976846008620517| null|\n",
      "|    min|   -2.1324060166211|    x|\n",
      "|    max| 1.6391310403186294|    z|\n",
      "+-------+-------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|                   n|abool|\n",
      "+--------------------+-----+\n",
      "| -0.5935304728163509|false|\n",
      "| -0.6537654714845256| true|\n",
      "|  1.1565401871181058|false|\n",
      "|-0.23502098371277275| true|\n",
      "| -0.7460054809035613| true|\n",
      "+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_n_abool = df.select('n','abool')\n",
    "df_n_abool.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|group|abool|\n",
      "+-----+-----+\n",
      "|    y|false|\n",
      "|    z| true|\n",
      "|    y|false|\n",
      "|    y| true|\n",
      "|    z| true|\n",
      "+-----+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_group_abool = df.select('group','abool')\n",
    "df_group_abool.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|group| True|\n",
      "+-----+-----+\n",
      "|    y|false|\n",
      "|    z| true|\n",
      "|    y|false|\n",
      "+-----+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_group_abool = df.select('group', df.abool.alias('True'))\n",
    "df_group_abool.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "|  1|    2|\n",
      "+---+-----+\n",
      "|  y|false|\n",
      "|  z| true|\n",
      "|  y|false|\n",
      "|  y| true|\n",
      "|  z| true|\n",
      "|  z|false|\n",
      "+---+-----+\n",
      "only showing top 6 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_group_abool = df.select(df.group.alias('1'),df.abool.alias('2'))\n",
    "df_group_abool.show(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Column Manipulation\n",
    "\n",
    "    1. Use the starter code above to re-create a spark dataframe. Store the\n",
    "       spark dataframe in a varaible named `df`\n",
    "\n",
    "    1. Use `.select` to add 4 to the `n` column. Show the results.\n",
    "\n",
    "    1. Subtract 5 from the `n` column and view the results.\n",
    "\n",
    "    1. Multiply the `n` column by 2. View the results along with the original\n",
    "       numbers.\n",
    "\n",
    "    1. Add a new column named `n2` that is the `n` value multiplied by -1. Show\n",
    "       the first 4 rows of your dataframe. You should see the original `n` value\n",
    "       as well as `n2`.\n",
    "\n",
    "    1. Add a new column named `n3` that is the n value squared. Show the first 5\n",
    "       rows of your dataframe. You should see both `n`, `n2`, and `n3`.\n",
    "\n",
    "    1. What happens when you run the code below?\n",
    "\n",
    "        ```python\n",
    "        df.group + df.abool\n",
    "        ```\n",
    "\n",
    "    1. What happens when you run the code below? What is the difference between\n",
    "       this and the previous code sample?\n",
    "\n",
    "        ```python\n",
    "        df.select(df.group + df.abool)\n",
    "        ```\n",
    "\n",
    "    1. Try adding various other columns together. What are the results of\n",
    "       combining the different data types?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, expr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_dataframe = pd.DataFrame(\n",
    "    {\n",
    "        \"n\": np.random.randn(20),\n",
    "        \"group\": np.random.choice(list(\"xyz\"), 20),\n",
    "        \"abool\": np.random.choice([True, False], 20),\n",
    "    }\n",
    ")\n",
    "\n",
    "spark = pyspark.sql.SparkSession.builder.getOrCreate()\n",
    "df = spark.createDataFrame(pandas_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+-----+\n",
      "|                  n|group|abool|\n",
      "+-------------------+-----+-----+\n",
      "|  0.727988440509268|    z| true|\n",
      "|0.28328764402352613|    y|false|\n",
      "| 0.4854567124399692|    z| true|\n",
      "|0.04009618088775551|    y| true|\n",
      "| 0.1317996612178238|    y| true|\n",
      "+-------------------+-----+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+-----+\n",
      "|                   n|group|abool|\n",
      "+--------------------+-----+-----+\n",
      "|   0.727988440509268|    z| true|\n",
      "| 0.28328764402352613|    y|false|\n",
      "|  0.4854567124399692|    z| true|\n",
      "| 0.04009618088775551|    y| true|\n",
      "|  0.1317996612178238|    y| true|\n",
      "|  0.2873562064464264|    z| true|\n",
      "| -1.3851913877896898|    x|false|\n",
      "|-0.32220016996103124|    y|false|\n",
      "|  -1.576401336370809|    y|false|\n",
      "| 0.18115222394868613|    x| true|\n",
      "| -0.2350901263489596|    y|false|\n",
      "|  -0.465898011124788|    x|false|\n",
      "|  0.4136179604698437|    z| true|\n",
      "|  1.1285645591978317|    y|false|\n",
      "| 0.12859791674477922|    z| true|\n",
      "| -0.3433205967783675|    y|false|\n",
      "| -0.8699232728719363|    x|false|\n",
      "|  1.2787735724336267|    x|false|\n",
      "| -1.2461674757326568|    x|false|\n",
      "| 0.29884689737797465|    x|false|\n",
      "+--------------------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_add_4 = df.select(df.n + 4, df.group, df.abool)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+-----+\n",
      "|                  n|group|abool|\n",
      "+-------------------+-----+-----+\n",
      "|  0.727988440509268|    z| true|\n",
      "|0.28328764402352613|    y|false|\n",
      "| 0.4854567124399692|    z| true|\n",
      "|0.04009618088775551|    y| true|\n",
      "| 0.1317996612178238|    y| true|\n",
      "+-------------------+-----+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sub_5 = df.select(df.n - 5, df.group, df.abool)\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+\n",
      "|            (n * 2)|                  n|\n",
      "+-------------------+-------------------+\n",
      "|  1.455976881018536|  0.727988440509268|\n",
      "| 0.5665752880470523|0.28328764402352613|\n",
      "| 0.9709134248799384| 0.4854567124399692|\n",
      "|0.08019236177551102|0.04009618088775551|\n",
      "| 0.2635993224356476| 0.1317996612178238|\n",
      "+-------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_X2 = df.select(df.n * 2, df.n)\n",
    "df_X2.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Spark SQL\n",
    "\n",
    "    1. Use the starter code above to re-create a spark dataframe.\n",
    "    1. Turn your dataframe into a table that can be queried with spark SQL. Name\n",
    "       the table `my_df`. Answer the rest of the questions in this section with\n",
    "       a spark sql query (`spark.sql`) against `my_df`. After each step, view\n",
    "       the first 7 records from the dataframe.\n",
    "    1. Write a query that shows all of the columns from your dataframe.\n",
    "    1. Write a query that shows just the `n` and `abool` columns from the\n",
    "       dataframe.\n",
    "    1. Write a query that shows just the `n` and `group` columns. Rename the\n",
    "       `group` column to `g`.\n",
    "    1. Write a query that selects `n`, and creates two new columns: `n2`, the\n",
    "       original `n` values halved, and `n3`: the original n values minus 1.\n",
    "    1. What happens if you make a SQL syntax error in your query?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_dataframe = pd.DataFrame(\n",
    "    {\n",
    "        \"n\": np.random.randn(20),\n",
    "        \"group\": np.random.choice(list(\"xyz\"), 20),\n",
    "        \"abool\": np.random.choice([True, False], 20),\n",
    "    }\n",
    ")\n",
    "\n",
    "spark = pyspark.sql.SparkSession.builder.getOrCreate()\n",
    "df = spark.createDataFrame(pandas_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"my_df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+-----+\n",
      "|                   n|group|abool|\n",
      "+--------------------+-----+-----+\n",
      "|  0.7458489961049468|    z|false|\n",
      "| 0.06497295596977953|    y|false|\n",
      "| 0.25012865092311865|    y|false|\n",
      "|0.008169109603518434|    y| true|\n",
      "| -0.7770347810255167|    z|false|\n",
      "|-0.06365780544492623|    x|false|\n",
      "|    0.07003372043635|    y| true|\n",
      "+--------------------+-----+-----+\n",
      "only showing top 7 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''SELECT * FROM my_df''').show(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|                   n|abool|\n",
      "+--------------------+-----+\n",
      "|  0.7458489961049468|false|\n",
      "| 0.06497295596977953|false|\n",
      "| 0.25012865092311865|false|\n",
      "|0.008169109603518434| true|\n",
      "| -0.7770347810255167|false|\n",
      "|-0.06365780544492623|false|\n",
      "|    0.07003372043635| true|\n",
      "+--------------------+-----+\n",
      "only showing top 7 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''SELECT n, abool\n",
    "             FROM my_df''').show(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+\n",
      "|                   n|  g|\n",
      "+--------------------+---+\n",
      "|  0.7458489961049468|  z|\n",
      "| 0.06497295596977953|  y|\n",
      "| 0.25012865092311865|  y|\n",
      "|0.008169109603518434|  y|\n",
      "| -0.7770347810255167|  z|\n",
      "|-0.06365780544492623|  x|\n",
      "|    0.07003372043635|  y|\n",
      "+--------------------+---+\n",
      "only showing top 7 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''SELECT n, group AS g\n",
    "             FROM my_df''').show(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a query that selects n, and creates two new columns: n2, the original n values halved, and n3: the original n values minus 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------------------+\n",
      "|                   n|                  2n|                 3n|\n",
      "+--------------------+--------------------+-------------------+\n",
      "|  0.7458489961049468|  0.3729244980524734|-0.2541510038950532|\n",
      "| 0.06497295596977953| 0.03248647798488977|-0.9350270440302204|\n",
      "| 0.25012865092311865| 0.12506432546155932|-0.7498713490768814|\n",
      "|0.008169109603518434|0.004084554801759217|-0.9918308903964815|\n",
      "| -0.7770347810255167|-0.38851739051275835|-1.7770347810255167|\n",
      "|-0.06365780544492623|-0.03182890272246...|-1.0636578054449262|\n",
      "|    0.07003372043635|   0.035016860218175|  -0.92996627956365|\n",
      "+--------------------+--------------------+-------------------+\n",
      "only showing top 7 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "          SELECT n, (n/2) AS 2n, (n-1) AS 3n\n",
    "          FROM my_df\n",
    "          ''').show(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Type casting\n",
    "\n",
    "    1. Use the starter code above to re-create a spark dataframe.\n",
    "\n",
    "    1. Use `.printSchema` to view the datatypes in your dataframe.\n",
    "\n",
    "    1. Use `.dtypes` to view the datatypes in your dataframe.\n",
    "\n",
    "    1. What is the difference between the two code samples below?\n",
    "\n",
    "        ```python\n",
    "        df.abool.cast('int')\n",
    "        ```\n",
    "\n",
    "        ```python\n",
    "        df.select(df.abool.cast('int')).show()\n",
    "        ```\n",
    "\n",
    "    1. Use `.select` and `.cast` to convert the `abool` column to an integer\n",
    "       type. View the results.\n",
    "    1. Convert the `group` column to a integer data type and view the results.\n",
    "       What happens?\n",
    "    1. Convert the `n` column to a integer data type and view the results. What\n",
    "       happens?\n",
    "    1. Convert the `abool` column to a string data type and view the results.\n",
    "       What happens?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_dataframe = pd.DataFrame(\n",
    "    {\n",
    "        \"n\": np.random.randn(20),\n",
    "        \"group\": np.random.choice(list(\"xyz\"), 20),\n",
    "        \"abool\": np.random.choice([True, False], 20),\n",
    "    }\n",
    ")\n",
    "\n",
    "spark = pyspark.sql.SparkSession.builder.getOrCreate()\n",
    "df = spark.createDataFrame(pandas_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- n: double (nullable = true)\n",
      " |-- group: string (nullable = true)\n",
      " |-- abool: boolean (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('n', 'double'), ('group', 'string'), ('abool', 'boolean')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<b'CAST(abool AS INT)'>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.abool.cast('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|abool|\n",
      "+-----+\n",
      "|    1|\n",
      "|    0|\n",
      "|    1|\n",
      "|    1|\n",
      "|    0|\n",
      "+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df.abool.cast('int')).show(5) # shows abool as int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|abool|\n",
      "+-----+\n",
      "|    1|\n",
      "|    0|\n",
      "|    1|\n",
      "|    1|\n",
      "|    0|\n",
      "+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df.abool.cast('int')).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|group|\n",
      "+-----+\n",
      "| null|\n",
      "| null|\n",
      "| null|\n",
      "| null|\n",
      "| null|\n",
      "+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df.group.cast('int')).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|  n|\n",
      "+---+\n",
      "|  0|\n",
      "|  0|\n",
      "|  0|\n",
      "|  0|\n",
      "| -1|\n",
      "+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df.n.cast('int')).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Built-in Functions\n",
    "\n",
    "    1. Use the starter code above to re-create a spark dataframe.\n",
    "    1. Import the necessary functions from `pyspark.sql.functions`\n",
    "    1. Find the highest `n` value.\n",
    "    1. Find the lowest `n` value.\n",
    "    1. Find the average `n` value.\n",
    "    1. Use `concat` to change the `group` column to say, e.g. \"Group: x\" or\n",
    "       \"Group: y\"\n",
    "    1. Use `concat` to combine the `n` and `group` columns to produce results\n",
    "       that look like this: \"x: -1.432\" or \"z: 2.352\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_dataframe = pd.DataFrame(\n",
    "    {\n",
    "        \"n\": np.random.randn(20),\n",
    "        \"group\": np.random.choice(list(\"xyz\"), 20),\n",
    "        \"abool\": np.random.choice([True, False], 20),\n",
    "    }\n",
    ")\n",
    "\n",
    "spark = pyspark.sql.SparkSession.builder.getOrCreate()\n",
    "df = spark.createDataFrame(pandas_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import concat,sum,avg,min,max,lit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------------------+------------------+\n",
      "|            max(n)|             min(n)|            avg(n)|\n",
      "+------------------+-------------------+------------------+\n",
      "|2.8579310286325224|-1.7150504069178603|0.2956113849624353|\n",
      "+------------------+-------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(max(df.n),min(df.n),avg(df.n)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+\n",
      "|concat(Group: , group)|\n",
      "+----------------------+\n",
      "|              Group: x|\n",
      "|              Group: x|\n",
      "|              Group: x|\n",
      "|              Group: x|\n",
      "|              Group: z|\n",
      "+----------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(concat(lit(\"Group: \"), col('group'))).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "| concat(group, :, n)|\n",
      "+--------------------+\n",
      "|x:0.4920813422832545|\n",
      "|x:0.6158480554333929|\n",
      "|x:-1.715050406917...|\n",
      "|x:-0.969704300304698|\n",
      "|z:0.2884450182676466|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(concat(col('group'),lit(':'),col('n'))).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Filter / Where\n",
    "\n",
    "    1. Use the starter code above to re-create a spark dataframe.\n",
    "    1. Use `.filter` or `.where` to select just the rows where the group is `y`\n",
    "       and view the results.\n",
    "    1. Select just the columns where the `abool` column is false and view the\n",
    "       results.\n",
    "    1. Find the columns where the `group` column is *not* `y`.\n",
    "    1. Find the columns where `n` is positive.\n",
    "    1. Find the columns where `abool` is true and the `group` column is `z`.\n",
    "    1. Find the columns where `abool` is true or the `group` column is `z`.\n",
    "    1. Find the columns where `abool` is false and `n` is less than 1\n",
    "    1. Find the columns where `abool` is false or `n` is less than 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_dataframe = pd.DataFrame(\n",
    "    {\n",
    "        \"n\": np.random.randn(20),\n",
    "        \"group\": np.random.choice(list(\"xyz\"), 20),\n",
    "        \"abool\": np.random.choice([True, False], 20),\n",
    "    }\n",
    ")\n",
    "\n",
    "spark = pyspark.sql.SparkSession.builder.getOrCreate()\n",
    "df = spark.createDataFrame(pandas_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+-----+\n",
      "|                  n|group|abool|\n",
      "+-------------------+-----+-----+\n",
      "| 0.7258017690577546|    y| true|\n",
      "|0.04869340042592003|    y| true|\n",
      "| 0.4568236077825877|    y|false|\n",
      "|-1.8988623219149174|    y| true|\n",
      "|-0.6449311328844708|    y| true|\n",
      "+-------------------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter((df.group=='y')).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+-----+\n",
      "|                   n|group|abool|\n",
      "+--------------------+-----+-----+\n",
      "| 0.06619528210571758|    x|false|\n",
      "|  0.4568236077825877|    y|false|\n",
      "|-0.11062430840489158|    x|false|\n",
      "|  1.4849541406216085|    z|false|\n",
      "|  0.5249092744030445|    z|false|\n",
      "+--------------------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter((df.abool==False)).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+-----+\n",
      "|                   n|group|abool|\n",
      "+--------------------+-----+-----+\n",
      "|-0.19149878964595465|    z| true|\n",
      "|  -1.756251409005848|    z| true|\n",
      "|  0.7376332949773831|    z| true|\n",
      "| -0.9995186526384939|    x| true|\n",
      "| -0.5777529197731882|    z| true|\n",
      "+--------------------+-----+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter((df.group!='y')).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+-----+\n",
      "|                  n|group|abool|\n",
      "+-------------------+-----+-----+\n",
      "| 0.7376332949773831|    z| true|\n",
      "|0.06619528210571758|    x|false|\n",
      "| 0.7258017690577546|    y| true|\n",
      "|0.04869340042592003|    y| true|\n",
      "|  1.395471272874804|    x| true|\n",
      "+-------------------+-----+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter((df.n>=0)).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+-----+\n",
      "|                   n|group|abool|\n",
      "+--------------------+-----+-----+\n",
      "|-0.19149878964595465|    z| true|\n",
      "|  -1.756251409005848|    z| true|\n",
      "|  0.7376332949773831|    z| true|\n",
      "| -0.5777529197731882|    z| true|\n",
      "|-0.21490021271307547|    z| true|\n",
      "+--------------------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df.abool==True).where(df.group=='z').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+-----+\n",
      "|                   n|group|abool|\n",
      "+--------------------+-----+-----+\n",
      "|-0.19149878964595465|    z| true|\n",
      "|  -1.756251409005848|    z| true|\n",
      "|  0.7376332949773831|    z| true|\n",
      "| -0.9995186526384939|    x| true|\n",
      "| -0.5777529197731882|    z| true|\n",
      "|  0.7258017690577546|    y| true|\n",
      "| -0.8083872118669346|    x| true|\n",
      "| 0.04869340042592003|    y| true|\n",
      "|   1.395471272874804|    x| true|\n",
      "| -0.7176677330788243|    x| true|\n",
      "|  0.7600323885350998|    x| true|\n",
      "|  1.4849541406216085|    z|false|\n",
      "| -1.8988623219149174|    y| true|\n",
      "| -0.8626565079722793|    x| true|\n",
      "|  0.5249092744030445|    z|false|\n",
      "|-0.21490021271307547|    z| true|\n",
      "| -0.6449311328844708|    y| true|\n",
      "+--------------------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter((df.abool==True) | (df.group=='z')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+-----+\n",
      "|                   n|group|abool|\n",
      "+--------------------+-----+-----+\n",
      "| 0.06619528210571758|    x|false|\n",
      "|  0.4568236077825877|    y|false|\n",
      "|-0.11062430840489158|    x|false|\n",
      "|  0.5249092744030445|    z|false|\n",
      "+--------------------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter((df.abool==False) & (df.n < 1)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+-----+\n",
      "|                   n|group|abool|\n",
      "+--------------------+-----+-----+\n",
      "|-0.19149878964595465|    z| true|\n",
      "|  -1.756251409005848|    z| true|\n",
      "|  0.7376332949773831|    z| true|\n",
      "| -0.9995186526384939|    x| true|\n",
      "| -0.5777529197731882|    z| true|\n",
      "+--------------------+-----+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter((df.abool==False) | (df.n < 1)).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. When / Otherwise\n",
    "\n",
    "    1. Use the starter code above to re-create a spark dataframe.\n",
    "    1. Use `when` and `.otherwise` to create a column that contains the text \"It\n",
    "       is true\" when `abool` is true and \"It is false\"\" when `abool` is false.\n",
    "    1. Create a column that contains 0 if n is less than 0, otherwise, the\n",
    "       original n value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_dataframe = pd.DataFrame(\n",
    "    {\n",
    "        \"n\": np.random.randn(20),\n",
    "        \"group\": np.random.choice(list(\"xyz\"), 20),\n",
    "        \"abool\": np.random.choice([True, False], 20),\n",
    "    }\n",
    ")\n",
    "\n",
    "spark = pyspark.sql.SparkSession.builder.getOrCreate()\n",
    "df = spark.createDataFrame(pandas_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+\n",
      "|abool|Truth_String|\n",
      "+-----+------------+\n",
      "| true|  It is True|\n",
      "| true|  It is True|\n",
      "| true|  It is True|\n",
      "| true|  It is True|\n",
      "|false| It is False|\n",
      "+-----+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df.abool,when(df.abool==True, \"It is True\").otherwise(\"It is False\").alias('Truth_String')).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+\n",
      "|                   n|   Positive_or_Zero|\n",
      "+--------------------+-------------------+\n",
      "| 0.14410354790123864|0.14410354790123864|\n",
      "|  0.1425318830306353| 0.1425318830306353|\n",
      "|-0.11753435775447621|                0.0|\n",
      "|-0.05898879678452567|                0.0|\n",
      "| 0.02907765554037852|0.02907765554037852|\n",
      "|-0.37866952724621417|                0.0|\n",
      "|  1.3118743067346454| 1.3118743067346454|\n",
      "|   2.305869416373438|  2.305869416373438|\n",
      "| 0.30323512962752536|0.30323512962752536|\n",
      "|  -0.680256611380646|                0.0|\n",
      "+--------------------+-------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df.n,when(df.n < 0, 0).otherwise(df.n).alias('Positive_or_Zero')).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Sorting\n",
    "\n",
    "    1. Use the starter code above to re-create a spark dataframe.\n",
    "    1. Sort by the `n` value.\n",
    "    1. Sort by the `group` value, both ascending and descending.\n",
    "    1. Sort by the group value first, then, within each group, sort by `n`\n",
    "       value.\n",
    "    1. Sort by `abool`, `group`, and `n`. Does it matter in what order you\n",
    "       specify the columns when sorting?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_dataframe = pd.DataFrame(\n",
    "    {\n",
    "        \"n\": np.random.randn(20),\n",
    "        \"group\": np.random.choice(list(\"xyz\"), 20),\n",
    "        \"abool\": np.random.choice([True, False], 20),\n",
    "    }\n",
    ")\n",
    "\n",
    "spark = pyspark.sql.SparkSession.builder.getOrCreate()\n",
    "df = spark.createDataFrame(pandas_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+-----+\n",
      "|                  n|group|abool|\n",
      "+-------------------+-----+-----+\n",
      "|-1.6862345883268082|    z|false|\n",
      "|-1.5097845963470908|    y| true|\n",
      "|-0.9401936269644735|    y|false|\n",
      "|-0.8305550322712313|    x|false|\n",
      "|-0.5833067771337954|    x|false|\n",
      "+-------------------+-----+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.sort(col('n')).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+-----+\n",
      "|                   n|group|abool|\n",
      "+--------------------+-----+-----+\n",
      "|  2.5429407553387087|    x| true|\n",
      "|-0.01034994586005614|    x| true|\n",
      "| -0.5617326120195298|    x| true|\n",
      "| 0.06646960447774532|    x| true|\n",
      "| -0.3528167116409259|    x| true|\n",
      "+--------------------+-----+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.sort(col('group')).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+-----+\n",
      "|                  n|group|abool|\n",
      "+-------------------+-----+-----+\n",
      "|0.43231867971899207|    z|false|\n",
      "| 1.1487581707355172|    z| true|\n",
      "| -0.424131466150611|    z|false|\n",
      "| 1.3990770458690214|    z|false|\n",
      "|-1.6862345883268082|    z|false|\n",
      "+-------------------+-----+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.sort(col('group').desc()).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+-----+\n",
      "|                   n|group|abool|\n",
      "+--------------------+-----+-----+\n",
      "| -0.8305550322712313|    x|false|\n",
      "| -0.5833067771337954|    x|false|\n",
      "| -0.5617326120195298|    x| true|\n",
      "|-0.40570448185281643|    x|false|\n",
      "| -0.3528167116409259|    x| true|\n",
      "+--------------------+-----+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.sort(df.group, df.n).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+-----+\n",
      "|                   n|group|abool|\n",
      "+--------------------+-----+-----+\n",
      "| -0.8305550322712313|    x|false|\n",
      "| -0.5833067771337954|    x|false|\n",
      "|-0.40570448185281643|    x|false|\n",
      "|-0.00618632342440...|    x|false|\n",
      "| -0.9401936269644735|    y|false|\n",
      "|  0.1357210885660363|    y|false|\n",
      "| 0.47037096676466916|    y|false|\n",
      "| -1.6862345883268082|    z|false|\n",
      "|  -0.424131466150611|    z|false|\n",
      "| 0.43231867971899207|    z|false|\n",
      "|  1.3990770458690214|    z|false|\n",
      "| -0.5617326120195298|    x| true|\n",
      "| -0.3528167116409259|    x| true|\n",
      "|-0.12370316213339579|    x| true|\n",
      "|-0.01034994586005614|    x| true|\n",
      "| 0.06646960447774532|    x| true|\n",
      "|  2.5429407553387087|    x| true|\n",
      "| -1.5097845963470908|    y| true|\n",
      "| -0.1140648455660212|    y| true|\n",
      "|  1.1487581707355172|    z| true|\n",
      "+--------------------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.sort(df.abool, df.group, df.n).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+-----+\n",
      "|                   n|group|abool|\n",
      "+--------------------+-----+-----+\n",
      "| -0.8305550322712313|    x|false|\n",
      "| -0.5833067771337954|    x|false|\n",
      "|-0.40570448185281643|    x|false|\n",
      "|-0.00618632342440...|    x|false|\n",
      "| -0.5617326120195298|    x| true|\n",
      "| -0.3528167116409259|    x| true|\n",
      "|-0.12370316213339579|    x| true|\n",
      "|-0.01034994586005614|    x| true|\n",
      "| 0.06646960447774532|    x| true|\n",
      "|  2.5429407553387087|    x| true|\n",
      "| -0.9401936269644735|    y|false|\n",
      "|  0.1357210885660363|    y|false|\n",
      "| 0.47037096676466916|    y|false|\n",
      "| -1.5097845963470908|    y| true|\n",
      "| -0.1140648455660212|    y| true|\n",
      "| -1.6862345883268082|    z|false|\n",
      "|  -0.424131466150611|    z|false|\n",
      "| 0.43231867971899207|    z|false|\n",
      "|  1.3990770458690214|    z|false|\n",
      "|  1.1487581707355172|    z| true|\n",
      "+--------------------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.sort(df.group, df.abool, df.n).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Aggregating\n",
    "\n",
    "    1. What is the average `n` value for each group in the `group` column?\n",
    "    1. What is the maximum `n` value for each group in the `group` column?\n",
    "    1. What is the minimum `n` value by `abool`?\n",
    "    1. What is the average `n` value for each unique combination of the `group`\n",
    "       and `abool` column?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_dataframe = pd.DataFrame(\n",
    "    {\n",
    "        \"n\": np.random.randn(20),\n",
    "        \"group\": np.random.choice(list(\"xyz\"), 20),\n",
    "        \"abool\": np.random.choice([True, False], 20),\n",
    "    }\n",
    ")\n",
    "\n",
    "spark = pyspark.sql.SparkSession.builder.getOrCreate()\n",
    "df = spark.createDataFrame(pandas_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|group|              avg(n)|\n",
      "+-----+--------------------+\n",
      "|    x| 0.33992568707345344|\n",
      "|    z|0.057891308286099265|\n",
      "|    y|-0.03014774155708...|\n",
      "+-----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(df.group).agg(avg(df.n)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------+\n",
      "|group|            max(n)|\n",
      "+-----+------------------+\n",
      "|    x|0.5349815199772688|\n",
      "|    z|2.1852914632153926|\n",
      "|    y| 1.135688982685717|\n",
      "+-----+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(df.group).agg(max(df.n)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+--------------------+\n",
      "|group|abool|              avg(n)|\n",
      "+-----+-----+--------------------+\n",
      "|    z|false| 0.28103916312352434|\n",
      "|    y|false| -0.8650616323073733|\n",
      "|    y| true|  0.1785807311304849|\n",
      "|    x|false|-0.00800202389283...|\n",
      "|    x| true|  0.5138895425565977|\n",
      "|    z| true|-0.05368261913261...|\n",
      "+-----+-----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(df.group,df.abool).agg(avg(df.n)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|group|              avg(n)|\n",
      "+-----+--------------------+\n",
      "| null| 0.07818670264340587|\n",
      "|    z|0.057891308286099265|\n",
      "|    y|-0.03014774155708...|\n",
      "|    x| 0.33992568707345344|\n",
      "+-----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.rollup(df.group).agg(avg(df.n)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
